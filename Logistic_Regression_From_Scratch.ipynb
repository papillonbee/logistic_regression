{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic_Regression_From_Scratch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/papillonbee/logistic_regression/blob/master/Logistic_Regression_From_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3h5evBSrNKc",
        "colab_type": "text"
      },
      "source": [
        "# Logistic Regression from Scratch with Gradient Descent and Newton's Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eYcWD-0TjIA",
        "colab_type": "text"
      },
      "source": [
        "Author: Papan Yongmalwong"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUpZJrgRrZGq",
        "colab_type": "text"
      },
      "source": [
        "## Introduction to Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRh_bAjnrboV",
        "colab_type": "text"
      },
      "source": [
        "### From Statistics Point of View"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JACP_2mbr9K-",
        "colab_type": "text"
      },
      "source": [
        "Given explanatory matrix $\\textbf{X}$ of size $n\\times p$ and response vector $\\textbf{Y}$ of size $n\\times1$\n",
        "\n",
        "where element $x_{ij}\\in\\textbf{X}$ takes any real value and element $y_i\\in\\textbf{Y}$ takes only binary value ($0$, $1$)\n",
        "\n",
        "$\\textbf{X}$ | | | |$\\textbf{Y}$\n",
        "--- | --- | --- | --- | --- \n",
        "$x_{1,1}$ | $x_{1,2}$ | $\\cdots$ | $x_{1,p}$ | $y_1$\n",
        "$x_{2,1}$ | $x_{2,2}$ | $\\cdots$ | $x_{2,p}$ | $y_2$\n",
        "$\\vdots$ | $\\vdots$ | $\\ddots$ | $\\vdots$ | $\\vdots$\n",
        "$x_{n,1}$ | $x_{n,2}$ | $\\cdots$ | $x_{n,p}$ | $y_n$\n",
        "\n",
        "The goal is to find a model that is able to predict the probability of $y^*=1$ given new explanatory variable $\\textbf{x}^*=\\begin{bmatrix}x_1*&x_2*&...&x_p*\\end{bmatrix}^T$.\n",
        "\n",
        "The model that is suitable for this problem is the logit model where we treat each response variable $Y_i$, conditioned on explanatory variable $\\textbf{x}_i=\\begin{bmatrix}x_{i1}&x_{i1}&\\cdots&x_{ip}\\end{bmatrix}^T$ as a random variable such that $Y_i|\\textbf{x}_i\\sim Bernoulli(p_i)$ where $p_i=Pr(Y_i=1|\\textbf{x}_i)=\\frac{e^{\\textbf{x}_i^T\\boldsymbol{\\beta}}}{1+e^{\\textbf{x}_i^T\\boldsymbol{\\beta}}}$ and that $Y_i|\\textbf{x}_i's$ are independent, for every row $i=1,...,n$.\n",
        "\n",
        "It is noteworthy that $Y_i|\\textbf{x}_i's$ are random variables, $\\textbf{x}_i's$ are constant $p\\times1$ vectors and $y_i's$ are constant binary values ($0$, $1$) and $\\boldsymbol{\\beta}$ is an unknown $p\\times1$ vector.\n",
        "\n",
        "The goal then becomes solving for coefficients $\\boldsymbol{\\beta}$ that satisfies the logit model assumption on $\\textbf{X}$ and $\\textbf{Y}$.\n",
        "\n",
        "Solving for $\\boldsymbol{\\beta}$ can be done by maximum likelihood estimation.\n",
        "\n",
        "$\\hat{\\boldsymbol{\\beta}}=argmax_{\\boldsymbol{\\beta}}\\mathcal{L}(\\boldsymbol{\\beta})$\n",
        "\n",
        "$\\mathcal{L}(\\boldsymbol{\\beta})$ $=Pr(Y_1=y_1,Y_2=y_2,...,Y_n=y_n|\\textbf{X})\\\\\n",
        "=\\prod_{i=1}^nPr(Y_i=y_i|\\textbf{x}_i)\\\\\n",
        "=\\prod_{i=1}^np_i^{y_i}(1-p_i)^{1-y_i}\\\\\n",
        "=\\prod_{i=1}^n(\\frac{e^{\\textbf{x}_i^T\\boldsymbol{\\beta}}}{1+e^{\\textbf{x}_i^T\\boldsymbol{\\beta}}})^{y_i}(\\frac{1}{1+e^{\\textbf{x}_i^T\\boldsymbol{\\beta}}})^{1-y_i}$\n",
        "\n",
        "$\\hat{\\boldsymbol{\\beta}}=argmin_{\\boldsymbol{\\beta}}-ln(\\mathcal{L}(\\boldsymbol{\\beta}))$\n",
        "\n",
        "$-ln(\\mathcal{L}(\\boldsymbol{\\beta}))$ $=\\sum_{i=1}^n[ln(1+e^{\\textbf{x}_i^T\\boldsymbol{\\beta}})-y_i\\textbf{x}_i^T\\boldsymbol{\\beta}]$\n",
        "\n",
        "Here, $\\hat{\\boldsymbol{\\beta}}$ is a maximum likelihood estimator for $\\boldsymbol{\\beta}$.\n",
        "\n",
        "There is no closed form for finding $\\hat{\\boldsymbol{\\beta}}$, thus iterative method is required.\n",
        "\n",
        "This [colab](https://colab.research.google.com/drive/1BxzwemHWPXJOFXoI7JBONQEohTjt7Msd) will guide you to finding $\\hat{\\boldsymbol{\\beta}}$ with $2$ iterative methods, which are **Gradient Descent** and **Newton's Method**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_5RGqCbr21j",
        "colab_type": "text"
      },
      "source": [
        "### From Operations Research Point of View"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7YfKKSu8cAA",
        "colab_type": "text"
      },
      "source": [
        "The logistic regression model can be viewed as a convex program. Solving for $\\boldsymbol{\\beta}$ by maximum likelihood estimation can be viewed as solving for optimal solution in a convex program.\n",
        "\n",
        "Let $f(\\boldsymbol{\\beta})=-ln(\\mathcal{L}(\\boldsymbol{\\beta}))$\n",
        "\n",
        "Decision variables: $\\boldsymbol{\\beta}\\in\\mathbb{R}^p$\n",
        "\n",
        "Convex program: $min\\ f(\\boldsymbol{\\beta})\\\\\n",
        "s.t. \\boldsymbol{\\beta}\\in\\mathbb{R}^p$\n",
        "\n",
        "It can be proven that the objective function $f(\\boldsymbol{\\beta})$ is a convex function, which is a requirement for a program to be convex.\n",
        "\n",
        "Both **Gradient Descent** and **Newton's Method** guarantee an optimal solution for $\\boldsymbol{\\beta}$ in a convex program."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9J93c-TU991A",
        "colab_type": "text"
      },
      "source": [
        "**Proof:** $f(\\boldsymbol{\\beta})=-ln(\\mathcal{L}(\\boldsymbol{\\beta}))$ is a convex function\n",
        "\n",
        "**Please feel free to skip the proof if you are not into mathematics*\n",
        "\n",
        "The proof requires a theorem which states that for a twice differentiable function $f:\\mathbb{R}^p\\rightarrow\\mathbb{R}$, $f(\\textbf{x})$ is a convex function if and only if $\\nabla^2f(\\textbf{x})$ is positive semi-definite\n",
        "\n",
        "From $f(\\boldsymbol{\\beta})=-ln(\\mathcal{L}(\\boldsymbol{\\beta}))=\\sum_{i=1}^n[ln(1+e^{\\textbf{x}_i^T\\boldsymbol{\\beta}})-y_i\\textbf{x}_i^T\\boldsymbol{\\beta}]$\n",
        "\n",
        "$\\nabla f(\\boldsymbol{\\beta})=\\sum_{i=1}^n(\\frac{e^{\\textbf{x}_i^T\\boldsymbol{\\beta}}}{1+e^{\\textbf{x}_i^T\\boldsymbol{\\beta}}}-y_i)\\textbf{x}_i$\n",
        "\n",
        "$\\nabla^2f(\\boldsymbol{\\beta})=\\sum_{i=1}^n\\frac{e^{\\textbf{x}_i^T\\boldsymbol{\\beta}}}{1+e^{\\textbf{x}_i^T\\boldsymbol{\\beta}}}\\frac{1}{1+e^{\\textbf{x}_i^T\\boldsymbol{\\beta}}}\\textbf{x}_i\\textbf{x}_i^T$\n",
        "\n",
        "Consider $\\textbf{A}_i=\\frac{e^{\\textbf{x}_i^T\\boldsymbol{\\beta}}}{1+e^{\\textbf{x}_i^T\\boldsymbol{\\beta}}}\\frac{1}{1+e^{\\textbf{x}_i^T\\boldsymbol{\\beta}}}\\textbf{x}_i\\textbf{x}_i^T$\n",
        "\n",
        "For non-zero vector $\\textbf{u}\\in\\mathbb{R}^p$, $\\textbf{u}^T\\textbf{A}_i\\textbf{u}$ $=\\textbf{u}^T(\\frac{e^{\\textbf{x}_i^T\\boldsymbol{\\beta}}}{1+e^{\\textbf{x}_i^T\\boldsymbol{\\beta}}}\\frac{1}{1+e^{\\textbf{x}_i^T\\boldsymbol{\\beta}}}\\textbf{x}_i\\textbf{x}_i^T)\\textbf{u}\\\\\n",
        "=\\frac{e^{\\textbf{x}_i^T\\boldsymbol{\\beta}}}{1+e^{\\textbf{x}_i^T\\boldsymbol{\\beta}}}\\frac{1}{1+e^{\\textbf{x}_i^T\\boldsymbol{\\beta}}}(\\textbf{x}_i^T\\textbf{u})^T(\\textbf{x}_i^T\\textbf{u})\\geq0$\n",
        "\n",
        "Thus, $\\textbf{A}_i$ is positive semi-definite. Moreover, the sum of positive semi-definite matrices is still a positive semi-definite matrix.\n",
        "\n",
        "Therefore, $\\nabla^2f(\\boldsymbol{\\beta})$ is positive semi-definite. As a result, $f(\\boldsymbol{\\beta})$ is a convex function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uy9EDI8xr6DH",
        "colab_type": "text"
      },
      "source": [
        "### From Neural Network Point of View"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VcabWERC5B8",
        "colab_type": "text"
      },
      "source": [
        "The logistic regression model can also be viewed as a neural network model with $1$ input layer and $1$ output layer (no hidden layer). Suppose there is explanatory matrix $\\textbf{X}$ of size $n\\times p$ and response vector $\\textbf{Y}$ of size $n\\times 1$. The input layer will consist of $k$ nodes (the number of columns in $\\textbf{X}$) and the output layer will consist of $1$ node (the number of columns in $\\textbf{Y}$). The activation function for the node in the output layer is the sigmoid function and the loss function for the model is the negative log-likelihood $f(\\boldsymbol{\\beta})=-ln(\\mathcal{L}(\\boldsymbol{\\beta}))$ (or binary cross-entropy). Solving for $\\boldsymbol{\\beta}$ by maximum likelihood estimation can be viewed as training this neural network with input $\\textbf{X}$ and output $\\textbf{Y}$, which produces the coefficients $\\boldsymbol{\\beta}$.\n",
        "\n",
        "Suppose you have $p=10$, the neural network architecture for logistic regression model will be as follows:\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/papillonbee/images/master/nn_logistic.png' alt=\"Drawing\" style=\"width: 200px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-QBr-BdVdQx",
        "colab_type": "text"
      },
      "source": [
        "## Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3Tb2eX8vCOd",
        "colab_type": "text"
      },
      "source": [
        "[Gradient descent](https://en.wikipedia.org/wiki/Gradient_descent)  is an iterative method that solves for minimum of a convex function.\n",
        "\n",
        "For a convex function $f(\\textbf{x})$, the step taken in each iteration is proportional to the steepest descent direction of $f(\\textbf{x})$, which is $-\\frac{\\nabla f(\\textbf{x})}{\\|\\nabla f(\\textbf{x})\\|}$. Please see **Appendix** for further detail on the intuition of gradient descent.\n",
        "\n",
        "$\\textbf{x}\\leftarrow\\textbf{x}-\\lambda\\nabla f(\\textbf{x})$\n",
        "\n",
        "$\\lambda$ is the step size in each iteration\n",
        "\n",
        "$\\nabla f(\\textbf{x})$ is the gradient of $f(\\textbf{x})$\n",
        "\n",
        "The method stops when  $f(\\textbf{x})$ is close to the minimum, or $\\nabla f(\\textbf{x})$ is close to $\\textbf{0}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdgGKcU73ORi",
        "colab_type": "text"
      },
      "source": [
        "The gradient descent algorithm that solves for $\\boldsymbol{\\beta}$ in logistic regression will be as follows:\n",
        "\n",
        "`while` $\\|\\nabla f(\\boldsymbol{\\beta})\\|$ `>` $\\epsilon$`:`\n",
        "\n",
        ">$\\boldsymbol{\\beta}\\leftarrow\\boldsymbol{\\beta}-\\lambda\\nabla f(\\boldsymbol{\\beta})$\n",
        "\n",
        "$\\lambda$ is the parameter you have to specify. It is recommended to take a small step; i.e. $\\lambda=0.001$ is used as an example below. Too small step size may result in a slow rate of convergence while too large step size may result in a divergence.\n",
        "\n",
        "Note that $\\lambda$ is not required to be fixed at every iteration. Instead, $\\lambda$ is allowed to change at every iteration and is chosen via [line search](https://en.wikipedia.org/wiki/Line_search). Here, $\\lambda$ is chosen to be fixed just for simplicity.\n",
        "\n",
        "The chosen stopping criterion here is the Euclidean norm of $\\nabla f(\\boldsymbol{\\beta})$ not exceeding a pre-specified tolerance $\\epsilon$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LyWwMDxVfLc",
        "colab_type": "text"
      },
      "source": [
        "## Newton's Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLqcQlklzRtk",
        "colab_type": "text"
      },
      "source": [
        "[Newton's method](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization) is an iterative method that solves for minimum of a convex function. Equivalently, the method solves for root of the derivative of the convex function. The idea behind this method is to use a quadratic approximate of the convex function and solve for its minimum at each step. Please see **Appendix** for further detail on the intuition of Newton's method.\n",
        "\n",
        "For a convex function $f(\\textbf{x})$, the step taken in each iteration is $-(\\nabla^2f(\\textbf{x}))^{-1}\\nabla f(\\textbf{x})$.\n",
        "\n",
        "$\\textbf{x}\\leftarrow\\textbf{x}-(\\nabla^2f(\\textbf{x}))^{-1}\\nabla f(\\textbf{x})$\n",
        "\n",
        "$\\nabla f(\\textbf{x})$ is the gradient of $f(\\textbf{x})$\n",
        "\n",
        "$\\nabla^2f(\\textbf{x})$ is the Hessian matrix of $f(\\textbf{x})$\n",
        "\n",
        "The method stops when  $f(\\textbf{x})$ is close to the minimum, or $\\nabla f(\\textbf{x})$ is close to $\\textbf{0}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nqZtviO3RO6",
        "colab_type": "text"
      },
      "source": [
        "The Newton's method algorithm that solves for $\\boldsymbol{\\beta}$ in logistic regression will be as follows:\n",
        "\n",
        "`while` $\\|\\nabla f(\\boldsymbol{\\beta})\\|$ `>` $\\epsilon$`:`\n",
        "\n",
        ">$\\boldsymbol{\\beta}\\leftarrow\\boldsymbol{\\beta}-(\\nabla^2f(\\boldsymbol{\\beta}))^{-1}\\nabla f(\\boldsymbol{\\beta})$\n",
        "\n",
        "The chosen stopping criterion here is the Euclidean norm of $\\nabla f(\\boldsymbol{\\beta})$ not exceeding a pre-specified tolerance $\\epsilon$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8AYFt9iUnHR",
        "colab_type": "text"
      },
      "source": [
        "## Implementation of Logistic Regression from Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76q3kwiHM0PC",
        "colab_type": "text"
      },
      "source": [
        "### Generate Dataset $\\textbf{X}$ and $\\textbf{Y}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqQqNa8hQOf4",
        "colab_type": "text"
      },
      "source": [
        "*   $\\textbf{X}$ is a matrix of size $2,\\!484\\times 2$\n",
        "*   $\\textbf{Y}$ is a vector of size $2,\\!484\\times 1$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KFwJ7larB89",
        "colab_type": "code",
        "outputId": "2f1f0f0c-9f59-49bf-fb1e-5d9e7d2c37e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "X = np.matrix(np.ones(2484)).T\n",
        "X = np.append(X,np.matrix([0]*1379+[2]*638+[4]*213+[5]*254).T,axis=1)\n",
        "Y = np.matrix([1]*24+[0]*1355+[1]*35+[0]*603+[1]*21+[0]*192+[1]*30+[0]*224).T\n",
        "\n",
        "print(X)\n",
        "print(Y)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " ...\n",
            " [1. 5.]\n",
            " [1. 5.]\n",
            " [1. 5.]]\n",
            "[[1]\n",
            " [1]\n",
            " [1]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBUkJmY9RhRe",
        "colab_type": "text"
      },
      "source": [
        "### Define $f(\\boldsymbol{\\beta})$, $\\nabla f(\\boldsymbol{\\beta})$, and $\\nabla^2f(\\boldsymbol{\\beta})$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CP6pctPzGlu4",
        "colab_type": "text"
      },
      "source": [
        "Please see **Appendix** for compact notation of $f(\\boldsymbol{\\beta})$, $\\nabla f(\\boldsymbol{\\beta})$, and $\\nabla^2f(\\boldsymbol{\\beta})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwnb0Z8b4BHR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f(beta):\n",
        "  return np.ravel(np.ones(len(Y))*(np.log(1+np.exp(X*beta)))-Y.T*X*beta)[0]\n",
        "\n",
        "def nabla_f(beta):\n",
        "  return X.T*(1/(1+1/np.exp(X*beta))-Y)\n",
        "\n",
        "def nabla2_f(beta):\n",
        "  return X.T*(np.diag(np.ravel(np.exp(X*beta)/np.power(1+np.exp(X*beta),2)))*X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvPeJI6gRv80",
        "colab_type": "text"
      },
      "source": [
        "### Implement Gradient Descent Algorithm ($\\lambda=0.001$)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZh-QbXWV2kM",
        "colab_type": "text"
      },
      "source": [
        "With initial guess $\\boldsymbol{\\beta}=\\begin{bmatrix}0&0\\end{bmatrix}^T$, the algorithm converges to $\\|\\nabla f(\\boldsymbol{\\beta})\\|\\leq10^{-10}$ at $771$ iterations\n",
        "\n",
        "$\\boldsymbol{\\beta}=\\begin{bmatrix}-3.8662481&0.39733662\\end{bmatrix}^T$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuyjUUqp4KEg",
        "colab_type": "code",
        "outputId": "13b5561c-cf66-4976-fce5-bbf99a9d5771",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        }
      },
      "source": [
        "beta = np.matrix(np.zeros(X.shape[1])).T\n",
        "TOL = np.power(10.,-10)\n",
        "lam = 0.001\n",
        "counter = 0\n",
        "\n",
        "while np.linalg.norm(nabla_f(beta)) > TOL:\n",
        "  counter += 1\n",
        "  beta -= lam*nabla_f(beta)\n",
        "  \n",
        "  if counter%100 == 0:\n",
        "    print('iter =',counter)\n",
        "    print(beta)\n",
        "    print('norm =',np.linalg.norm(nabla_f(beta)))\n",
        "    \n",
        "print('iter =',counter)\n",
        "print(beta)\n",
        "print('norm =',np.linalg.norm(nabla_f(beta)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter = 100\n",
            "[[-3.82924142]\n",
            " [ 0.38809993]]\n",
            "norm = 1.3219213776192646\n",
            "iter = 200\n",
            "[[-3.86511786]\n",
            " [ 0.39705501]]\n",
            "norm = 0.039785775221124586\n",
            "iter = 300\n",
            "[[-3.86621308]\n",
            " [ 0.3973279 ]]\n",
            "norm = 0.0012322422461308706\n",
            "iter = 400\n",
            "[[-3.86624702]\n",
            " [ 0.39733635]]\n",
            "norm = 3.819830459640027e-05\n",
            "iter = 500\n",
            "[[-3.86624807]\n",
            " [ 0.39733661]]\n",
            "norm = 1.1841422505568608e-06\n",
            "iter = 600\n",
            "[[-3.8662481 ]\n",
            " [ 0.39733662]]\n",
            "norm = 3.670822322298812e-08\n",
            "iter = 700\n",
            "[[-3.8662481 ]\n",
            " [ 0.39733662]]\n",
            "norm = 1.1379578802548557e-09\n",
            "iter = 771\n",
            "[[-3.8662481 ]\n",
            " [ 0.39733662]]\n",
            "norm = 9.680410123994959e-11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vq6hlWqZUApB",
        "colab_type": "text"
      },
      "source": [
        "### Implement Newton's Method Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5y-FvQDWCij",
        "colab_type": "text"
      },
      "source": [
        "With initial guess $\\boldsymbol{\\beta}=\\begin{bmatrix}0&0\\end{bmatrix}^T$, the algorithm converges to $\\|\\nabla f(\\boldsymbol{\\beta})\\|\\leq10^{-10}$ at $7$ iterations\n",
        "\n",
        "$\\boldsymbol{\\beta}=\\begin{bmatrix}-3.8662481&0.39733662\\end{bmatrix}^T$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OOt4_Va4Pev",
        "colab_type": "code",
        "outputId": "40562768-b101-47bd-98bd-e2e668d94ea6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        }
      },
      "source": [
        "beta = np.matrix(np.zeros(X.shape[1])).T\n",
        "TOL = np.power(10.,-10)\n",
        "counter = 0\n",
        "\n",
        "while np.linalg.norm(nabla_f(beta)) > TOL:\n",
        "  counter += 1\n",
        "  beta -= np.linalg.inv(nabla2_f(beta))*nabla_f(beta)\n",
        "  \n",
        "  print('iter =',counter)\n",
        "  print(beta)\n",
        "  print('norm =',np.linalg.norm(nabla_f(beta)))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter = 1\n",
            "[[-1.93251069]\n",
            " [ 0.08015202]]\n",
            "norm = 344.0674429365813\n",
            "iter = 2\n",
            "[[-2.91445219]\n",
            " [ 0.19953576]]\n",
            "norm = 78.8473161236118\n",
            "iter = 3\n",
            "[[-3.56822777]\n",
            " [ 0.32983402]]\n",
            "norm = 14.865386138822815\n",
            "iter = 4\n",
            "[[-3.83189761]\n",
            " [ 0.38960211]]\n",
            "norm = 1.5569804528923392\n",
            "iter = 5\n",
            "[[-3.86575804]\n",
            " [ 0.39722704]]\n",
            "norm = 0.0224489498794286\n",
            "iter = 6\n",
            "[[-3.866248 ]\n",
            " [ 0.3973366]]\n",
            "norm = 4.632921725232224e-06\n",
            "iter = 7\n",
            "[[-3.8662481 ]\n",
            " [ 0.39733662]]\n",
            "norm = 5.444006648261367e-13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aG8DmRlKUJJg",
        "colab_type": "text"
      },
      "source": [
        "### Check Coefficients from `sklearn.linear_model.LogisticRegression`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxJuRbKYMiLX",
        "colab_type": "text"
      },
      "source": [
        "$\\boldsymbol{\\beta}=\\begin{bmatrix}-3.86624885&0.39733451\\end{bmatrix}^T$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoZuwIcCAy7m",
        "colab_type": "code",
        "outputId": "33b149cb-032c-40da-8a0c-e7b6a23221db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression(solver='lbfgs',C=1e9,fit_intercept=False).fit(X,np.ravel(Y))\n",
        "\n",
        "np.set_printoptions(suppress=True)\n",
        "print(clf.coef_)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-3.86624885  0.39733451]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq8UU2103Kme",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzmautIO3QfM",
        "colab_type": "text"
      },
      "source": [
        "In comparing the efficiency of Newton's method and gradient descent for solving the coefficients $\\boldsymbol{\\beta}\\in\\mathbb{R}^{p}$ in logistic regression model given explanatory matrix $\\textbf{X}\\in\\mathbb{R}^{n\\times p}$ and response vector $\\textbf{Y}\\in\\mathbb{R}^{n}$, the runtime complexity for each iteration in Newton's method is $\\mathcal{O}(np(n+p)+p^3)$ while the runtime complexity for each iteration in gradient descent is $\\mathcal{O}(np)$. The memory complexity for each iteration in Newton's method is $\\mathcal{O}(p^2)$ while the memory complexity for each iteration in gradient descent is $\\mathcal{O}(p)$. It can be seen that Newton's method requires more number of operations and more amount of memory than gradient descent at each iteration. However, Newton's method requires less iterations to converge to minimum because it uses the knowledge of the second derivative $\\nabla^2f(\\boldsymbol{\\beta})$ at each iteration while gradient descent only uses the knowledge of the first derivative $\\nabla f(\\boldsymbol{\\beta})$ at each iteration. This is the trade-off between the complexity and speed of convergence of Newton's method and gradient descent. In fact, the Newton's method is very fast to converge yet requires high amount of complexity for computing the inverse of Hessian matrix $\\nabla^2f(\\boldsymbol{\\beta})$.  Computing the inverse of Hessian matrix can be unavailable for a very large dataset. In real world, ones ought to approximate the inverse of Hessian matrix, leading to less complexity, in exchange for more iterations to converge to minimum. The methods are called [the Quasi-Newton methods](https://en.wikipedia.org/wiki/Quasi-Newton_method). It is noteworthy that the solver specified in `sklearn.linear_model.LogisticRegression` is `solver='lbfgs'` or [Limited-memory BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS), an iterative method in the family of Quasi-Newton methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8zd3FmR_0Xb",
        "colab_type": "text"
      },
      "source": [
        "## Appendix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TASn_gcpeokr",
        "colab_type": "text"
      },
      "source": [
        "### Intuition of Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKPFw729eqpA",
        "colab_type": "text"
      },
      "source": [
        "Gradient descent is an iterative method that solves for minimum of a convex function. The method moves the guessing point to the next guessing point in the direction that $f(\\textbf{x})$ decreases the fastest.\n",
        "\n",
        "To move in the direction that $f(\\textbf{x})$ decreases the fastest, consider the theorem\n",
        "\n",
        ">For $f(\\textbf{x})\\in\\mathbb{R}$, a unit vector $-\\frac{\\nabla f(\\textbf{x}_0)}{\\|f(\\textbf{x}_0)\\|}$ is the steepest descent direction of $f(\\textbf{x})$ at $\\textbf{x}=\\textbf{x}_0$\n",
        "\n",
        "In gradient descent, in order to find the minimum of the convex function $f(\\textbf{x})$, we start from our initial guess $\\textbf{x}_0$,  move $\\textbf{x}_0$ to the next point in the direction that $f(\\textbf{x})$ decreases the fastest $\\textbf{x}_0-\\lambda_0\\nabla f(\\textbf{x}_0)$, let $\\textbf{x}_1=\\textbf{x}_0-\\lambda_0\\nabla f(\\textbf{x}_0)$ be the next guess, move $\\textbf{x}_1$ to the next point in the direction that $f(\\textbf{x})$ decreases the fastest $\\textbf{x}_1-\\lambda_1\\nabla f(\\textbf{x}_1)$, let $\\textbf{x}_2=\\textbf{x}_1-\\lambda_1\\nabla f(\\textbf{x}_1)$ be the next guess, and do these steps recursively until $f(\\textbf{x}_i)$ is close to minimum. Therefore, gradient descent algorithm can be written in a recursive form as follows:\n",
        "\n",
        "$\\textbf{x}_{i+1}\\leftarrow\\textbf{x}_i-\\lambda_i\\nabla f(\\textbf{x}_i)$ for arbitrary value of $\\lambda_i$\n",
        "\n",
        "At each iteration, $\\lambda_i$ is chosen such that $f(\\textbf{x}_i-\\lambda_i\\nabla f(\\textbf{x}_i))$ is smallest. This can be done via [line search](https://en.wikipedia.org/wiki/Line_search).\n",
        "\n",
        "You can see that gradient descent moves from point $\\textbf{x}_i$ to point $\\textbf{x}_{i+1}$ in the direction $-\\lambda_i\\nabla f(\\textbf{x}_i)$, which is proportional to $-\\frac{\\nabla f(\\textbf{x}_i)}{\\|f(\\textbf{x}_i)\\|}$, the steepest descent direction of $f(\\textbf{x})$ at $\\textbf{x}=\\textbf{x}_i$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41nWSOHRWwP7",
        "colab_type": "text"
      },
      "source": [
        "### Intuition of Newton's Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gF_1EnkMWyyW",
        "colab_type": "text"
      },
      "source": [
        "Newton's method is an iterative method that solves for minimum of a convex function. The method uses the quadratic approximate of the convex function and solves for its minimum at each step.\n",
        "\n",
        "To approximate the convex function with quadratic function at each step, consider Taylor series of $f(\\textbf{x})$ centered at $\\textbf{x}=\\textbf{x}_0$\n",
        "\n",
        "$f(\\textbf{x})=f(\\textbf{x}_0)+(\\textbf{x}-\\textbf{x}_0)^T\\nabla f(\\textbf{x}_0)+\\frac{1}{2}(\\textbf{x}-\\textbf{x}_0)^T\\nabla^2f(\\textbf{x}_0)(\\textbf{x}-\\textbf{x}_0)+...$\n",
        "\n",
        "Approximate $f(\\textbf{x})$ to polynomial of degree $2$\n",
        "\n",
        "$f(\\textbf{x})\\approx f(\\textbf{x}_0)+(\\textbf{x}-\\textbf{x}_0)^T\\nabla f(\\textbf{x}_0)+\\frac{1}{2}(\\textbf{x}-\\textbf{x}_0)^T\\nabla^2f(\\textbf{x}_0)(\\textbf{x}-\\textbf{x}_0)$\n",
        "\n",
        "Take gradient on both sides\n",
        "\n",
        "$\\nabla f(\\textbf{x})\\approx\\nabla f(\\textbf{x}_0)+\\frac{1}{2}(\\nabla^2f(\\textbf{x}_0)+\\nabla^2f(\\textbf{x}_0)^T)(\\textbf{x}-\\textbf{x}_0)$\n",
        "\n",
        "$\\nabla f(\\textbf{x})\\approx\\nabla f(\\textbf{x}_0)+\\nabla^2f(\\textbf{x}_0)(\\textbf{x}-\\textbf{x}_0)$\n",
        "\n",
        "Let $\\nabla f(\\textbf{x})=0$ and solve for $\\textbf{x}$\n",
        "\n",
        "$\\nabla f(\\textbf{x})=0\\approx\\nabla f(\\textbf{x}_0)+\\nabla^2f(\\textbf{x}_0)(\\textbf{x}-\\textbf{x}_0)$\n",
        "\n",
        "$\\textbf{x}\\approx\\textbf{x}_0-(\\nabla^2f(\\textbf{x}_0))^{-1}\\nabla f(\\textbf{x}_0)$\n",
        "\n",
        "Here, $\\textbf{x}$ is the minimum from the quadratic approximate of $f(\\textbf{x})$ centered at $\\textbf{x}=\\textbf{x}_0$.\n",
        "\n",
        "In Newton's method, we let $\\textbf{x}_1=\\textbf{x}$ be the next point for Taylor series to be centered at, we then find the minimum from the quadratic approximate of $f(\\textbf{x})$ centered at $\\textbf{x}=\\textbf{x}_1$ and obtain the minimum $\\textbf{x}$, set $\\textbf{x}_2=\\textbf{x}$ and do these steps recursively until $f(\\textbf{x}_i)$ is close to minimum. Therefore, Newton's method algorithm can be written in a recursive form as follows:\n",
        "\n",
        "$\\textbf{x}_{i+1}\\leftarrow\\textbf{x}_i-(\\nabla^2f(\\textbf{x}_i))^{-1}\\nabla f(\\textbf{x}_i)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWzG7v4_O_Da",
        "colab_type": "text"
      },
      "source": [
        "### Compact Notation of $f(\\boldsymbol{\\beta})$, $\\nabla f(\\boldsymbol{\\beta})$, and $\\nabla^2f(\\boldsymbol{\\beta})$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyOJBqUdF3np",
        "colab_type": "text"
      },
      "source": [
        "The gradient and Hessian matrix of the negative log-likelihood  $f(\\boldsymbol{\\beta})=-ln(\\mathcal{L}(\\boldsymbol{\\beta}))$ are derived as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIcUsID9_16O",
        "colab_type": "text"
      },
      "source": [
        "$f(\\boldsymbol{\\beta})$ $=-ln(\\mathcal{L}(\\boldsymbol{\\beta}))\\\\=\\sum_{i=1}^n[ln(1+e^{\\textbf{x}_i^T\\boldsymbol{\\beta}})-y_i\\textbf{x}_i^T\\boldsymbol{\\beta}]$\n",
        "\n",
        "$\\frac{\\partial f(\\boldsymbol{\\beta})}{\\partial\\beta_j}$ $=\\sum_{i=1}^n[\\frac{e^{\\textbf{x}_i^T\\boldsymbol{\\beta}}}{1+e^{\\textbf{x}_i^T\\boldsymbol{\\beta}}}x_{ij}-y_ix_{ij}],\\ \\forall j=1,2,...,p$\n",
        "\n",
        "$\\therefore\\nabla f(\\boldsymbol{\\beta})$ $=\\sum_{i=1}^n(\\frac{e^{\\textbf{x}_i^T\\boldsymbol{\\beta}}}{1+e^{\\textbf{x}_i^T\\boldsymbol{\\beta}}}-y_i)\\textbf{x}_i$\n",
        "\n",
        "$\\frac{\\partial^2 f(\\boldsymbol{\\beta})}{\\partial\\beta_j\\partial\\beta_k}$ $=\\sum_{i=1}^n\\frac{e^{\\textbf{x}_i^T\\boldsymbol{\\beta}}}{1+e^{\\textbf{x}_i^T\\boldsymbol{\\beta}}}\\frac{1}{1+e^{\\textbf{x}_i^T\\boldsymbol{\\beta}}}x_{ij}x_{ik},\\ \\forall j,k=1,2,...,p$\n",
        "\n",
        "$\\therefore\\nabla^2f(\\boldsymbol{\\beta})$ $=\\sum_{i=1}^n\\frac{e^{\\textbf{x}_i^T\\boldsymbol{\\beta}}}{(1+e^{\\textbf{x}_i^T\\boldsymbol{\\beta}})^2}\\textbf{x}_i\\textbf{x}_i^T$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UxryqsBE9xx",
        "colab_type": "text"
      },
      "source": [
        "From above, $f(\\boldsymbol{\\beta})$, $\\nabla f(\\boldsymbol{\\beta})$, and $\\nabla^2f(\\boldsymbol{\\beta})$ can be written in a more compact form as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36VQzLHdGB6e",
        "colab_type": "text"
      },
      "source": [
        "$f(\\boldsymbol{\\beta})=\\textbf{1}^Tln(1+e^{\\textbf{X}\\boldsymbol{\\beta}})-\\textbf{Y}^T\\textbf{X}\\boldsymbol{\\beta}$\n",
        "\n",
        "$\\nabla f(\\boldsymbol{\\beta})=\\textbf{X}^T(\\frac{e^{\\textbf{X}\\boldsymbol{\\beta}}}{1+e^{\\textbf{X}\\boldsymbol{\\beta}}}-\\textbf{Y})$\n",
        "\n",
        "$\\nabla^2f(\\boldsymbol{\\beta})=\\textbf{X}^Tdiag(\\frac{e^{\\textbf{X}\\boldsymbol{\\beta}}}{1+e^{\\textbf{X}\\boldsymbol{\\beta}}}\\circ\\frac{1}{1+e^{\\textbf{X}\\boldsymbol{\\beta}}})\\textbf{X}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOlVF37FLZyk",
        "colab_type": "text"
      },
      "source": [
        "Here, \n",
        "\n",
        "$\\textbf{X}\\in\\mathbb{R}^{n\\times p}$\n",
        "\n",
        "$\\textbf{Y}\\in\\mathbb{R}^n$\n",
        "\n",
        "$\\boldsymbol{\\beta}\\in\\mathbb{R}^p$\n",
        "\n",
        "$\\textbf{1}\\in\\mathbb{R}^n$\n",
        "\n",
        "$ln(1+e^{\\textbf{X}\\boldsymbol{\\beta}})\\in\\mathbb{R}^n$\n",
        "\n",
        "$\\frac{e^{\\textbf{X}\\boldsymbol{\\beta}}}{1+e^{\\textbf{X}\\boldsymbol{\\beta}}}\\in\\mathbb{R}^n$\n",
        "\n",
        "$\\frac{1}{1+e^{\\textbf{X}\\boldsymbol{\\beta}}}\\in\\mathbb{R}^n$\n",
        "\n",
        "$diag(\\frac{e^{\\textbf{X}\\boldsymbol{\\beta}}}{1+e^{\\textbf{X}\\boldsymbol{\\beta}}}\\circ\\frac{1}{1+e^{\\textbf{X}\\boldsymbol{\\beta}}})\\in\\mathbb{R}^{n\\times n}$\n",
        "\n",
        "$diag()$ is a notation for diagonal matrix with entries specified inside the parenthesis\n",
        "\n",
        "$\\circ$ is a notation for a [Hadamard product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)) (element-wise product)"
      ]
    }
  ]
}